{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f7bf9b",
   "metadata": {},
   "source": [
    "# English–Spanish Translation Dataset Example\n",
    "\n",
    "This Jupyter Notebook demonstrates how to build and use a neural network with a Transformer architecture to translate from English to Spanish.\n",
    "\n",
    "To begin, download the `spa.txt` dataset file into your working directory. This file contains paired English–Spanish sentences. You can download it from the following link:\n",
    "\n",
    "[Download English–Spanish Dataset](https://www.kaggle.com/datasets/lonnieqin/englishspanish-translation-dataset/data)\n",
    "\n",
    "Make sure the downloaded file is placed in the same directory as this notebook to ensure the dataset loads correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "from src.tensor import Tensor, op\n",
    "from src.optimizer import Adam\n",
    "from src.scheduler import CosineScheduler\n",
    "from src.preprocessing import DataLoader, Tokenizer, train_test_split\n",
    "from src.encode import OneHotEncoder\n",
    "from src.structure import Layer, Embedding, Transformer, Dense\n",
    "from src.loss import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(Layer):\n",
    "    \"\"\"A simple sequence-to-sequence model using Transformer architecture.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab: set | tuple | list,\n",
    "        tgt_vocab: set | tuple | list,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        dim_feedforward=128,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        self.d_model = d_model\n",
    "        self.src_tokenizer = Tokenizer(list(src_vocab))\n",
    "        self.tgt_tokenizer = Tokenizer(list(tgt_vocab))\n",
    "\n",
    "        self.src_embedding = Embedding(d_model, len(self.src_tokenizer.vocab))\n",
    "        self.tgt_embedding = Embedding(d_model, len(self.tgt_tokenizer.vocab))\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            dim_feedforward,\n",
    "            self.src_embedding,\n",
    "            self.tgt_embedding,\n",
    "            dropout,\n",
    "        )\n",
    "        self.out_proj = Dense(len(self.tgt_tokenizer.vocab))\n",
    "\n",
    "        self.encoder = OneHotEncoder(tuple(range(len(self.tgt_tokenizer.vocab))))\n",
    "\n",
    "    def __call__(self, src: Tensor, tgt: Tensor | None = None, max_len: int = -1) -> Tensor:\n",
    "        if Tensor.training:\n",
    "            assert tgt is not None, \"Target must not be None when training.\"\n",
    "\n",
    "        src_pad_mask = self._generate_padding_mask(src, self.src_tokenizer.word2idx[Tokenizer.PAD])\n",
    "\n",
    "        if tgt is not None:\n",
    "            return self.out_proj(self.transformer(src, tgt, attn_mask_encoder=src_pad_mask, attn_mask_decoder=None))\n",
    "        return self.transformer.generate(\n",
    "            src,\n",
    "            tgt,\n",
    "            max_len=max_len,\n",
    "            out_proj=self.out_proj,\n",
    "            sos_token_id=self.tgt_tokenizer.word2idx[Tokenizer.SOS],\n",
    "            eos_token_id=self.tgt_tokenizer.word2idx[Tokenizer.EOS],\n",
    "            pad_token_id=self.tgt_tokenizer.word2idx[Tokenizer.PAD],\n",
    "            attn_mask_encoder=src_pad_mask,\n",
    "        )\n",
    "\n",
    "    @Tensor.train()\n",
    "    def train(self, src: list[list[str]], tgt: list[list[str]], batch_size: int) -> None:\n",
    "        src_train, tgt_train, src_test, tgt_test = train_test_split(src, tgt)\n",
    "\n",
    "        loss = CategoricalCrossentropy(ignore_token_id=self.tgt_tokenizer.word2idx[\"<pad>\"])\n",
    "        loader = DataLoader(src_train, tgt_train, batch_size=batch_size)\n",
    "        optimizer = Adam(list(self.parameters), CosineScheduler(1e-3, max_steps=4000), b2=0.98)\n",
    "        optimizer.params_requires_grad(True)\n",
    "\n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs + 1):\n",
    "            for src_batch, tgt_batch in loader:\n",
    "                src_batch, tgt_batch, tgt_out = self._tokenize(src_batch, tgt_batch)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self(src_batch, tgt_batch)\n",
    "\n",
    "                outputs = op.softmax(outputs)\n",
    "\n",
    "                target_one_hot = self.encoder(tgt_out)\n",
    "\n",
    "                batch_size, seq_len, vocab_size = outputs.shape\n",
    "                predicted = op.reshape(outputs, (batch_size * seq_len, vocab_size))\n",
    "                expected = op.reshape(target_one_hot, (batch_size * seq_len, vocab_size))\n",
    "\n",
    "                loss_val = loss(predicted, expected)\n",
    "\n",
    "                loss_val.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            print(f\"\\nEpoch {epoch}, Train Loss: {loss_val.mean().item():.4f}\")\n",
    "            self.validate(src_test, tgt_test)\n",
    "\n",
    "    @Tensor.no_grad()\n",
    "    def validate(self, src_val, tgt_val):\n",
    "        print(\"Validating...\")\n",
    "\n",
    "        val_loader = DataLoader(src_val, tgt_val, batch_size=1)\n",
    "\n",
    "        src_batch, tgt_batch, _ = self._tokenize(*next(val_loader))\n",
    "\n",
    "        Tensor.set_training(False)\n",
    "        predicted = self(src_batch, None, max_len=50)\n",
    "        Tensor.set_training(True)\n",
    "\n",
    "        src = map(self.src_tokenizer.decode, src_batch)\n",
    "        decoded_true = map(self.tgt_tokenizer.decode, tgt_batch)\n",
    "        decoded_pred = map(self.tgt_tokenizer.decode, predicted)\n",
    "\n",
    "        for src_, truth, guess in zip(src, decoded_true, decoded_pred):\n",
    "            print(\"Source:   \", \" \".join(src_))\n",
    "            print(\"Target:   \", \" \".join(truth))\n",
    "            print(\"Predicted:\", \" \".join(guess))\n",
    "            print()\n",
    "\n",
    "    def _tokenize(self, src: list[list[str]], tgt: list[list[str]]) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        max_src = max(len(s) for s in src) + 1  # +1 for <eos>\n",
    "        max_tgt = max(len(s) for s in tgt) + 2  # +1 sos +1 eos\n",
    "        src_ids = [self.src_tokenizer(s, max_src, add_sos=False, add_eos=True) for s in src]\n",
    "\n",
    "        tgt_in_ids, tgt_out_ids = [], []\n",
    "        for sentence in tgt:\n",
    "            in_ids = self.tgt_tokenizer(sentence, max_tgt, add_sos=True, add_eos=False)\n",
    "            out_ids = self.tgt_tokenizer(sentence, max_tgt, add_sos=False, add_eos=True)\n",
    "            tgt_in_ids.append(in_ids)\n",
    "            tgt_out_ids.append(out_ids)\n",
    "\n",
    "        return (\n",
    "            Tensor(src_ids, dtype=int, requires_grad=True),\n",
    "            Tensor(tgt_in_ids, dtype=int, requires_grad=True),\n",
    "            Tensor(tgt_out_ids, dtype=int, requires_grad=False),\n",
    "        )\n",
    "\n",
    "    def _generate_padding_mask(self, tokens: Tensor, pad_token_id: int) -> Tensor:\n",
    "        return Tensor((tokens == pad_token_id).astype(int) * -1e-9, dtype=float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_puntuation = re.compile(r\"([?.!,¿¡])\")\n",
    "remove_extra_spaces = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def clean_sentence(sentence: str) -> list[str]:\n",
    "    sentence = re.sub(remove_puntuation, r\" \\1 \", sentence)\n",
    "    sentence = re.sub(remove_extra_spaces, \" \", sentence)\n",
    "    return sentence.strip().lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39419a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor.set_default_device(\"cuda\")\n",
    "\n",
    "with open(\"examples/spa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip().split(\"\\t\")[:2] for line in f if \"\\t\" in line]\n",
    "\n",
    "src_data, tgt_data = [], []\n",
    "for src, tgt in lines:\n",
    "    src_data.append(clean_sentence(src))\n",
    "    tgt_data.append(clean_sentence(tgt))\n",
    "\n",
    "src_vocab = set(chain.from_iterable(src_data))\n",
    "tgt_vocab = set(chain.from_iterable(tgt_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSeq2Seq(\n",
    "    src_vocab=src_vocab,\n",
    "    tgt_vocab=tgt_vocab,\n",
    "    d_model=256,\n",
    "    nhead=4,\n",
    "    dim_feedforward=256,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "model.train(src_data, tgt_data, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
