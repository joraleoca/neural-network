{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15568d536549c2d",
   "metadata": {},
   "source": [
    "# MNIST Dataset Example\n",
    "\n",
    "This Jupyter Notebook demonstrates how to build and use a neural network with convolutional and pooling layers to process and classify images from the MNIST dataset.\n",
    "\n",
    "## Download the Dataset\n",
    "\n",
    "To begin, you need to download the `mnist.npz` dataset file into your working directory. This file contains pre-split training and testing sets of the MNIST dataset. You can download it from the following link:\n",
    "\n",
    "[Download MNIST Dataset](https://s3.amazonaws.com/img-datasets/mnist.npz)\n",
    "\n",
    "Ensure that the downloaded file is placed in the same directory as this notebook to load the dataset seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src import BaseModel\n",
    "\n",
    "from src.loss import CategoricalCrossentropy\n",
    "from src.optimizer import Adam\n",
    "from src.tensor import Tensor, op\n",
    "from src.structure import Dense, Convolution, Dropout, MaxPool, Flatten, LeakyRelu, Softmax\n",
    "from src.encode import OneHotEncoder\n",
    "from src.preprocessing import DataLoader, min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fd5a3c11f56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './mnist.npz'\n",
    "\n",
    "with np.load(path, allow_pickle=True) as f:\n",
    "    x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n",
    "    x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n",
    "\n",
    "x_train = min_max_scaler(np.array(x_train), 0, 1)\n",
    "x_test = min_max_scaler(np.array(x_test), 0, 1)\n",
    "\n",
    "x_train = op.expand_dims(x_train, axis=1)\n",
    "x_test = op.expand_dims(x_test, axis=1)\n",
    "\n",
    "classes = tuple(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc32943ab5b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseModel):\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = [\n",
    "            Convolution(channels=8, kernel_shape=(3, 3), padding=1),\n",
    "            LeakyRelu(),\n",
    "            MaxPool(channels=8, filter_shape=(2, 2)),\n",
    "            Convolution(channels=16, kernel_shape=(3, 3), padding=1),\n",
    "            LeakyRelu(),\n",
    "            MaxPool(channels=16, filter_shape=(2, 2)),\n",
    "            Convolution(channels=32, kernel_shape=(3, 3), padding=1),\n",
    "            LeakyRelu(),\n",
    "            Flatten(),\n",
    "            Dense(64),\n",
    "            LeakyRelu(),\n",
    "            Dropout(p=0.1),\n",
    "            Dense(10),\n",
    "            Softmax(),\n",
    "        ]\n",
    "\n",
    "        self.encoder = OneHotEncoder(classes)\n",
    "\n",
    "    def __call__(self, inputs: Tensor) -> Tensor:\n",
    "        return inputs.sequential(self.layers)\n",
    "\n",
    "    def train(self, steps: int) -> None:\n",
    "        data = DataLoader(x_train, y_train, batch_size=32)\n",
    "\n",
    "        opt = Adam(self.parameters(), 1e-6)\n",
    "\n",
    "        loss_func = CategoricalCrossentropy()\n",
    "\n",
    "        for step in range(steps):\n",
    "            x, y = next(data)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loss = loss_func(self(x), self.encoder(y))\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(f\"loss: {loss.mean()}\")\n",
    "\n",
    "        print(f\"train: {self.evaluate(x_train, y_train)}\")\n",
    "\n",
    "    @Tensor.no_grad()\n",
    "    def evaluate(self, data: Tensor, expected: Tensor) -> float:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            total += 1\n",
    "\n",
    "            if self.encoder.decode(self(data[i])) == expected[i].item():\n",
    "                correct += 1\n",
    "\n",
    "        return correct / total\n",
    "\n",
    "nn = Model()\n",
    "\n",
    "nn.train(100_000)\n",
    "\n",
    "print(f\"test: {nn.evaluate(x_test, y_test)}, train: {nn.evaluate(x_train, y_train)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
