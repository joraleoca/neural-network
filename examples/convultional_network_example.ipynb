{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15568d536549c2d",
   "metadata": {},
   "source": [
    "# MNIST Dataset Example\n",
    "\n",
    "This Jupyter Notebook demonstrates how to build and use a neural network with convolutional and pooling layers to process and classify images from the MNIST dataset.\n",
    "\n",
    "## Download the Dataset\n",
    "\n",
    "To begin, you need to download the `mnist.npz` dataset file into your working directory. This file contains pre-split training and testing sets of the MNIST dataset. You can download it from the following link:\n",
    "\n",
    "[Download MNIST Dataset](https://s3.amazonaws.com/img-datasets/mnist.npz)\n",
    "\n",
    "Ensure that the downloaded file is placed in the same directory as this notebook to load the dataset seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src import NeuralNetwork\n",
    "\n",
    "from src.loss import CategoricalCrossentropy\n",
    "from src.optimizer import Adam\n",
    "from src.scheduler import CosineScheduler\n",
    "from src.config import FeedForwardConfig, TrainingConfig\n",
    "from src.structure import Dense, Convolution, Dropout, MaxPool, Flatten, LeakyRelu, Softmax\n",
    "from src.encode import OneHotEncoder\n",
    "from src.preprocessing import min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fd5a3c11f56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './mnist.npz'\n",
    "\n",
    "with np.load(path, allow_pickle=True) as f:\n",
    "    x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n",
    "    x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n",
    "\n",
    "x_train = min_max_scaler(np.array(x_train), 0, 1)\n",
    "x_test = min_max_scaler(np.array(x_test), 0, 1)\n",
    "\n",
    "classes = tuple(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc32943ab5b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = FeedForwardConfig(\n",
    "    network_structure= [\n",
    "        Convolution(channels=8, kernel_shape=(3, 3), padding=1),\n",
    "        LeakyRelu(),\n",
    "        MaxPool(channels=8, filter_shape=(2, 2)),\n",
    "        Convolution(channels=16, kernel_shape=(3, 3), padding=1),\n",
    "        LeakyRelu(),\n",
    "        MaxPool(channels=16, filter_shape=(2, 2)),\n",
    "        Convolution(channels=32, kernel_shape=(3, 3), padding=1),\n",
    "        LeakyRelu(),\n",
    "        Flatten(),\n",
    "        Dense(64),\n",
    "        LeakyRelu(),\n",
    "        Dropout(p=0.1),\n",
    "        Dense(10),\n",
    "        Softmax(),\n",
    "    ],\n",
    "    classes=classes,\n",
    "    encoder=OneHotEncoder,\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(config)\n",
    "\n",
    "nn.train(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    config=TrainingConfig(\n",
    "        lr=CosineScheduler(learning_rate=0.0001, min_lr=1e-7, max_steps=100),\n",
    "        patience_stop=100,\n",
    "        loss=CategoricalCrossentropy(),\n",
    "        optimizer=Adam(),\n",
    "        batch_size=32,\n",
    "        debug=True,\n",
    "        store=None,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"test: {nn.evaluate(x_test, y_test)}, train: {nn.evaluate(x_train, y_train)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
